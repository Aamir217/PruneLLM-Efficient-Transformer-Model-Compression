# PruneLLM-Efficient-Transformer-Model-Compression
PruneLLM is a PyTorch-based project for efficient compression of transformer-based large language models using dependency-aware structured pruning with Torch-Pruning. The project demonstrates how to reduce model size and inference cost while preserving model structure and performance.
